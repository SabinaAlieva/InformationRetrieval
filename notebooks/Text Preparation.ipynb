{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### дополнительная предобработка текста\n",
    "\n",
    "уже есть:\n",
    "- Перевод всех букв в тексте в нижний или верхний регистры\n",
    "- Удаление пунктуации\n",
    "- Токенизация \n",
    "\n",
    "сделать:\n",
    "- Удаление цифр (чисел) или замена на текстовый эквивалент \n",
    "- Удаление стоп слов\n",
    "- Стемминг\n",
    "- Лемматизация\n",
    "- Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_hot.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
       "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "грустно\n",
      "зависимость\n",
      "хороший\n",
      "приводить\n",
      "альтернатива\n"
     ]
    }
   ],
   "source": [
    "# лемматизация\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "words = ['грустно', 'зависимость', 'хорошему', 'приводит', 'альтернатив']\n",
    "for word in words:\n",
    "    p = morph.parse(word)[0]\n",
    "    print(p.normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "устойчив\n"
     ]
    }
   ],
   "source": [
    "# стемминг\n",
    "# -*- coding: utf-8 -*-\n",
    "# https://gist.github.com/Kein1945/9111512\n",
    "import re\n",
    "\n",
    "class Porter:\n",
    "    PERFECTIVEGROUND =  re.compile(u\"((ив|ивши|ившись|ыв|ывши|ывшись)|((?<=[ая])(в|вши|вшись)))$\")\n",
    "    REFLEXIVE = re.compile(u\"(с[яь])$\")\n",
    "    ADJECTIVE = re.compile(u\"(ее|ие|ые|ое|ими|ыми|ей|ий|ый|ой|ем|им|ым|ом|его|ого|ему|ому|их|ых|ую|юю|ая|яя|ою|ею)$\")\n",
    "    PARTICIPLE = re.compile(u\"((ивш|ывш|ующ)|((?<=[ая])(ем|нн|вш|ющ|щ)))$\")\n",
    "    VERB = re.compile(u\"((ила|ыла|ена|ейте|уйте|ите|или|ыли|ей|уй|ил|ыл|им|ым|ен|ило|ыло|ено|ят|ует|уют|ит|ыт|ены|ить|ыть|ишь|ую|ю)|((?<=[ая])(ла|на|ете|йте|ли|й|л|ем|н|ло|но|ет|ют|ны|ть|ешь|нно)))$\")\n",
    "    NOUN = re.compile(u\"(а|ев|ов|ие|ье|е|иями|ями|ами|еи|ии|и|ией|ей|ой|ий|й|иям|ям|ием|ем|ам|ом|о|у|ах|иях|ях|ы|ь|ию|ью|ю|ия|ья|я)$\")\n",
    "    RVRE = re.compile(u\"^(.*?[аеиоуыэюя])(.*)$\")\n",
    "    DERIVATIONAL = re.compile(u\".*[^аеиоуыэюя]+[аеиоуыэюя].*ость?$\")\n",
    "    DER = re.compile(u\"ость?$\")\n",
    "    SUPERLATIVE = re.compile(u\"(ейше|ейш)$\")\n",
    "    I = re.compile(u\"и$\")\n",
    "    P = re.compile(u\"ь$\")\n",
    "    NN = re.compile(u\"нн$\")\n",
    "\n",
    "    def stem(word):\n",
    "        word = word.lower()\n",
    "        word = word.replace(u'ё', u'е')\n",
    "        m = re.match(Porter.RVRE, word)\n",
    "        if m.groups():\n",
    "            pre = m.group(1)\n",
    "            rv = m.group(2)\n",
    "            temp = Porter.PERFECTIVEGROUND.sub('', rv, 1)\n",
    "            if temp == rv:\n",
    "                rv = Porter.REFLEXIVE.sub('', rv, 1)\n",
    "                temp = Porter.ADJECTIVE.sub('', rv, 1)\n",
    "                if temp != rv:\n",
    "                    rv = temp\n",
    "                    rv = Porter.PARTICIPLE.sub('', rv, 1)\n",
    "                else:\n",
    "                    temp = Porter.VERB.sub('', rv, 1)\n",
    "                    if temp == rv:\n",
    "                        rv = Porter.NOUN.sub('', rv, 1)\n",
    "                    else:\n",
    "                        rv = temp\n",
    "            else:\n",
    "                rv = temp\n",
    "\n",
    "            rv = Porter.I.sub('', rv, 1)\n",
    "\n",
    "            if re.match(Porter.DERIVATIONAL, rv):\n",
    "                rv = Porter.DER.sub('', rv, 1)\n",
    "\n",
    "            temp = Porter.P.sub('', rv, 1)\n",
    "            if temp == rv:\n",
    "                rv = Porter.SUPERLATIVE.sub('', rv, 1)\n",
    "                rv = Porter.NN.sub(u'н', rv, 1)\n",
    "            else:\n",
    "                rv = temp\n",
    "            word = pre+rv\n",
    "        return word\n",
    "    stem=staticmethod(stem)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(Porter.stem(u'устойчивость'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "# import gensim\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\138904\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "text = 'Давайте рассмотрим, пожалуй самый простой пример использования библиотеки NLTK токенизацию'.split(\" \")\n",
    "# print(word_tokenize(text))\n",
    "\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Давайте', 'рассмотрим,', 'пожалуй', 'самый', 'простой', 'пример', 'использования', 'библиотеки', 'NLTK', 'токенизацию']\n"
     ]
    }
   ],
   "source": [
    "tokens = [i for i in text if (i not in stop_words)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['дава', 'рассмотрим,', 'пожал', 'сам', 'прост', 'пример', 'использован', 'библиотек', 'NLTK', 'токенизац']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\") \n",
    "\n",
    "stem = [stemmer.stem(word) for word in tokens]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\138904\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Installing mystem to C:\\Users\\138904/.local/bin\\mystem.exe from http://download.cdn.yandex.net/mystem/mystem-3.1-win-64bit.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Examples    \n",
    "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")\n",
    "#> 'сказать видеть кто-то наступать грабли разочаровывать натравлять'\n",
    "\n",
    "preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\")\n",
    "#> 'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['асфальт', 'мимо', 'цемент', 'избегать', 'зевака', 'аплодисменты', 'обитатель', 'спальный', 'аррондисман']\n"
     ]
    }
   ],
   "source": [
    "lem_text = preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\").split(\" \")\n",
    "print(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['асфальт', 'мим', 'цемент', 'избега', 'зевак', 'аплодисмент', 'обитател', 'спальн', 'аррондисма']\n"
     ]
    }
   ],
   "source": [
    "stem = [stemmer.stem(word) for word in lem_text]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "коронавирусный\n",
      "Коронавирусный\n",
      "коронный\n",
      "Коронный\n"
     ]
    }
   ],
   "source": [
    "text = \"Как приготовить банановые оладьи:Для начала разомните банан вилкой, превратите его в однородную кашицу.\\\n",
    "Добавьте к банану яйцо. Перемешайте. Добавьте молоко и муку. Размешайте до однородного состояния.\"\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"Коронавирусный\"))\n",
    "print(lemmatizer.lemmatize(\"Коронавирусный\", wordnet.VERB))\n",
    "\n",
    "print(stemmer.stem(\"Коронный\"))\n",
    "print(lemmatizer.lemmatize(\"Коронный\", wordnet.VERB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'коронный'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Коронный\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'коронный'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Коронный\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "коронный\n"
     ]
    }
   ],
   "source": [
    "p = morph.parse(\"Коронный\")[0]\n",
    "print(p.normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'корон'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Porter.stem(u'Коронный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "корон\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"корон\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I like this movie, movie is funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  is  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1   0     1     0      2     0    0     1    0\n",
       "1        0      0     1   0   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271787</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412640</td>\n",
       "      <td>0.412640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334067</td>\n",
       "      <td>0.523381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        is        it      like      love  \\\n",
       "0  0.000000  0.425807  0.000000  0.425807  0.000000  0.335711  0.000000   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.523381  0.000000  0.000000  0.000000  0.412640  0.412640  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.414289  0.000000  0.525473   \n",
       "\n",
       "      movie      nice       one      this       was  \n",
       "0  0.671422  0.000000  0.000000  0.271787  0.000000  \n",
       "1  0.553492  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.334067  0.523381  \n",
       "3  0.000000  0.525473  0.525473  0.000000  0.000000  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
