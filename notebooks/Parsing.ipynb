{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализация web-crawler для сбора коллекции документов с указанного ресурса. \n",
    "\n",
    "Выбор формата хранения коллекции - MongoDB? Forlder with files? Relation DataBase\n",
    "\n",
    "### Характеристики коллекции:\n",
    "- Объем собранной коллекции не менее 100 тысяч уникальных документов \n",
    "- Размер текста для каждого документа должен быть не менее 2000 символов\n",
    "- Механизм очистки документов от не релевантной информации, например:\n",
    "    - html теги\n",
    "    - ссылки на сторонние или внутренние ресурсы сайта\n",
    "    - ссылки на изображения и видео\n",
    "    - знаки препинания\n",
    "- Выделение признаков документа\n",
    "    - автор\n",
    "    - тематические теги\n",
    "    - рейтинг\n",
    "    - репосты\n",
    "    - дата публикации \n",
    "    - другая метаинформация, предоставляемая ресурсом.\n",
    "\n",
    "### Результатом сдачи является:\n",
    "- код web-crawler\n",
    "- инструмент очистки текстов\n",
    "- исходная коллекция документов\n",
    "- коллекция документов после очистки\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIKABU = \"https://pikabu.ru/\"\n",
    "PIKABU_HOT = \"https://pikabu.ru/\"\n",
    "PIKABU_BEST = \"https://pikabu.ru/best\"\n",
    "PIKABU_FRESH = \"https://pikabu.ru/new\"\n",
    "PIKABU_GROUP = \"https://pikabu.ru/communities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг через BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\138904\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "path_webdriver = 'C:/Users/Public/Programms/chromedriver_win32/chromedriver.exe'\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')  # для открытия headless-браузера\n",
    "\n",
    "browser = webdriver.Chrome(executable_path=path_webdriver, chrome_options=options)\n",
    "browser.get(PIKABU);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = browser.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(main_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(main_page, 'lxml')\n",
    "table = soup.findChildren('article')\n",
    "# table = soup.findChildren('div', {'class': 'story__content-inner'})\n",
    "\n",
    "print(len(table))\n",
    "# print(table[0].prettify())\n",
    "\n",
    "# print(table[2].find('div', {'class': 'story__content-inner'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['div', 'div', 'div', 'div', 'svg', 'use', 'div', 'div', 'svg', 'use', 'div', 'div', 'div', 'span', 'div', 'header', 'h2', 'a', 'div', 'div', 'div', 'p', 'p', 'div', 'a', 'a', 'a', 'a', 'a', 'div', 'div', 'a', 'span', 'svg', 'use', 'span', 'div', 'span', 'svg', 'use', 'span', 'div', 'svg', 'use', 'div', 'svg', 'use', 'div', 'div', 'div', 'div', 'a', 'div', 'time', 'a', 'div', 'span', 'img']\n",
      "['div', 'div']\n"
     ]
    }
   ],
   "source": [
    "root = table[0]\n",
    "\n",
    "# перебор всех тегов\n",
    "\n",
    "recursive_childs = [e.name for e in root.recursiveChildGenerator() if e.name is not None]  \n",
    "print(recursive_childs)\n",
    "\n",
    "\n",
    "# При помощи атрибута children можно вывести все дочерние теги.\n",
    "\n",
    "root_childs = [e.name for e in root.children if e.name is not None]\n",
    "print(root_childs)\n",
    "\n",
    "\n",
    "# список всех потомков (дочерних элементов всех уровней) рассматриваемого тега.\n",
    "\n",
    "root_childs = [e.name for e in root.descendants if e.name is not None]\n",
    "# print(root_childs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[моё] Коронавирус Общепит Кризис Список Деньги\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(table[2].find('div', {'class': 'story__tags tags'}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(root.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг через requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как получить user-agent описано здесь https://habr.com/ru/post/280238/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 YaBrowser/20.3.1.197 Yowser/2.5 Safari/537.36'\n",
    "}\n",
    "r = requests.get('https://pikabu.ru//', headers = headers)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"story__content-inner\">\n",
      "<div class=\"story-block story-block_type_text\"><p>Заходит как-то Путин в бар и говорит:</p><p>- Всем пива за счет заведения!</p></div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "articles = soup.find('div', {'class': 'story__content-inner'})\n",
    "print(articles)\n",
    "# lxml \n",
    "tree = html.fromstring(r.text)\n",
    "#film_list_lxml = tree.xpath('//div[@class = \"profileFilmsList\"]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"ul\", id=\"mylist\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-svg no-js\" dir=\"ltr\" lang=\"ru\">\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# удаление тегов\n",
    "test = BeautifulSoup(r.text, 'lxml')\n",
    "div = test.select_one(\"head\")\n",
    "div.decompose()\n",
    "div = test.select_one(\"body\")\n",
    "div.decompose()\n",
    "print(test.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление всех тегов подобного типа\n",
    "\n",
    "test = BeautifulSoup(r.text, 'lxml')\n",
    "div = test.select(\"meta\")\n",
    "for d in div:\n",
    "    d.decompose()\n",
    "div = test.select(\"link\")\n",
    "for d in div:\n",
    "    d.decompose()\n",
    "# print(test.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    r = requests.get(url, headers=headers)    # Получаем метод Response\n",
    "    return r.text            # Вернем данные объекта text\n",
    "\n",
    "# print(get_html(PIKABU))\n",
    "\n",
    "def get_head(html):\n",
    "    soup = BeautifulSoup(html, 'lxml') #  Создаем сам объект , передаем в него наш код страницы (html) \n",
    "    return soup\n",
    "    \n",
    "soup = get_head(get_html(PIKABU))\n",
    "\n",
    "# print(soup.prettify())\n",
    "## Теперь с помощью метода fine() найдем блок со статьями, <div id=\"section-content\"> . \n",
    "## Уже в этом блоке найдем все теги заголовка <h1> методом fine_all() в которых собственно и содержится название статьи.\n",
    "\n",
    "head = soup.find_all('div', {'class':'auth__field'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(html_page, filename):\n",
    "    html_file = open(filename, \"w\", encoding='UTF-8')\n",
    "    html_file.write(html_page)\n",
    "    html_file.close()\n",
    "    \n",
    "save_html(soup.prettify(),\"mainpage.html\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article():\n",
    "    \n",
    "    def __init__(self, _title, _author, _date, _text, _rating, _tags, _amount_comments, _have_image):\n",
    "        self.title = _title\n",
    "        self.author = _author\n",
    "        self.date = _date\n",
    "        self.text = _text\n",
    "        self.rating = _rating\n",
    "        self.tags = _tags\n",
    "        self.amount_comments = _amount_comments\n",
    "        self.have_image = _have_image\n",
    "        \n",
    "    def showInfo():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = Article(\"title\",\"Sabina\",\"30.03\",\"TestJsonObjectSerialize\",100,['tag1', 'tag2'], 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = json.dumps(art.__dict__) \n",
    "# print(json.dumps(art.__dict__, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create new data_file.json file with write mode using file i/o operation \n",
    "with open('json_file.json', \"w\") as file_write:\n",
    "    # write json data into file\n",
    "    json.dump(art.__dict__, file_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'title', 'author': 'Sabina', 'date': '30.03', 'text': 'TestJsonObjectSerialize', 'rating': 100, 'tags': ['tag1', 'tag2'], 'amount_comments': 10, 'have_image': False}\n"
     ]
    }
   ],
   "source": [
    "with open('json_file.json') as file_object:\n",
    "        # store file data in object\n",
    "    data = json.load(file_object)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n"
     ]
    }
   ],
   "source": [
    "with open('json_file.json') as file_object:\n",
    "        # store file data in object\n",
    "    data = json.load(file_object)\n",
    "    print(data['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
