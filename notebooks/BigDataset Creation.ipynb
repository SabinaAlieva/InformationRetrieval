{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'datapikabu_dataset_new.csv'\n",
    "filename_processed = 'datapikabu_dataset_new_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"^\\s+|\\n|\\r|\\t|\\s+$\", \"\", text) # отступы\n",
    "    text = re.sub(r'.[0-9]+', ' ', text) # цифры\n",
    "    text = re.sub(r'.[a-z]+', ' ', text) # английские символы\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text) # пунктуация\n",
    "    text = text.lower() # регистр\n",
    "    text = \" \".join([word for word in text.split(\" \") if (word not in stop_words)]) # стоп-слова\n",
    "    text = \" \".join([t for t in text.split(\" \") if len(t) > 0]) # лишние пробелы\n",
    "    text = \" \".join([morph.parse(word)[0].normal_form for word in text.split(\" \")]) # лемматизация\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enough = df[df['Text'].map(lambda x: len(str(x)) > 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filename = \"pikabu_dataset_longText.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enough.to_csv(new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В лоб на 2 дня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "df = df[df['Text'].map(lambda x: len(str(x)) > 1000)]\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    df.loc[index, 'Text'] = preprocessing(df.loc[index, 'Text'])\n",
    "    df.loc[index, 'Tags'] = preprocessing(df.loc[index, 'Tags'])\n",
    "    df.loc[index, 'Title'] = preprocessing(df.loc[index, 'Title'])\n",
    "\n",
    "df.to_csv(\"pikabu_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv(new_filename, inferSchema=True, header=True).toDF('Number', 'Number2', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_processed = \"pikabu_dataset_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "filename = \"hot_dataset.csv\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(filename, inferSchema=True, header=True).toDF('Number', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')\n",
    "\n",
    "prepr = F.udf(preprocessing, StringType())\n",
    "\n",
    "df_spark.withColumn('Text2', prepr(df_spark['Text'])) \\\n",
    ".toPandas().to_csv(\"hot_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.withColumn('Text', reduce(preprocessing, [col('Text')])) \\\n",
    ".withColumn('Title', reduce(preprocessing, [col('Title')])) \\\n",
    ".withColumn('Tags', reduce(preprocessing, [col('Tags')])) \\\n",
    ".toPandas().to_csv(filename_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "filename = \"hot_dataset.csv\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(filename, inferSchema=True, header=True).toDF('Number', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')\n",
    "\n",
    "ss = SparkSession.builder\n",
    "\n",
    "sparkSession = ss.getOrCreate()\n",
    "sc = sparkSession.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def rowwise_function(row):\n",
    "    row_dict = row.asDict()\n",
    "    row_dict['Text'] = preprocessing(row_dict['Text'])\n",
    "    row_dict['Tags'] = preprocessing(row_dict['Tags'])\n",
    "    row_dict['Title'] = preprocessing(row_dict['Title'])\n",
    "    newrow = Row(**row_dict)\n",
    "    return newrow\n",
    "\n",
    "\n",
    "dfSchema = StructType([\n",
    "    StructField('Number',IntegerType(), True), \n",
    "    StructField('FileName', StringType(), True), \n",
    "    StructField('Title', StringType(),True), \n",
    "    StructField('Link', StringType(), True),\n",
    "    StructField('ArticleId', IntegerType(), True), \n",
    "    StructField('Date', StringType(), True), \n",
    "    StructField('Views', IntegerType(), True),\n",
    "    StructField('Author', StringType(), True), \n",
    "    StructField('Tags', StringType(), True), \n",
    "    StructField('AmountComments', IntegerType(), True), \n",
    "    StructField('Rating', IntegerType(), True), \n",
    "    StructField('Text', StringType(), True)\n",
    "        ])\n",
    "\n",
    "spark_rdd = df_spark.rdd\n",
    "spark_rdd_new = spark_rdd.map(lambda row: rowwise_function(row))\n",
    "\n",
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new dfSchema)\n",
    "spark_rdd_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new, dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new.toPandas().to_csv(\"hot_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new.write.format('com.databricks.spark.csv').save('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "filename = \"hot_dataset.csv\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(filename, inferSchema=True, header=True).toDF('Number', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')\n",
    "\n",
    "ss = SparkSession.builder\n",
    "sparkSession = ss.getOrCreate()\n",
    "sc = sparkSession.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def rowwise_function(row):\n",
    "    row_dict = row.asDict()\n",
    "    row_dict['Text'] = preprocessing(row_dict['Text'])\n",
    "    row_dict['Tags'] = preprocessing(row_dict['Tags'])\n",
    "    row_dict['Title'] = preprocessing(row_dict['Title'])\n",
    "    newrow = Row(**row_dict)\n",
    "    return newrow\n",
    "\n",
    "\n",
    "dfSchema = StructType([\n",
    "    StructField('Number',IntegerType(), True), \n",
    "    StructField('FileName', StringType(), True), \n",
    "    StructField('Title', StringType(),True), \n",
    "    StructField('Link', StringType(), True),\n",
    "    StructField('ArticleId', StringType(), IntegerType(), True), \n",
    "    StructField('Date', StringType(), True), \n",
    "    StructField('Views', IntegerType(), True),\n",
    "    StructField('Author', StringType(), True), \n",
    "    StructField('Tags', StringType(), True), \n",
    "    StructField('AmountComments', IntegerType(), True), \n",
    "    StructField('Rating', IntegerType(), True), \n",
    "    StructField('Text', StringType(), True)\n",
    "        ])\n",
    "\n",
    "spark_rdd = df_spark.rdd\n",
    "spark_rdd_new = spark_rdd.map(lambda row: rowwise_function(row))\n",
    "\n",
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new, dfSchema)\n",
    "spark_rdd_new.show()\n",
    "#spark_rdd_new.toPandas().to_csv(\"hot_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"hot_dataset_processed.csv\"\n",
    "spark_rdd_new.repartition(1).write.csv(name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "filename = \"hot_dataset.csv\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(filename, inferSchema=True, header=True).toDF('Number', 'FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')\n",
    "\n",
    "ss = SparkSession.builder\n",
    "sparkSession = ss.getOrCreate()\n",
    "sc = sparkSession.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def rowwise_function(row):\n",
    "    row_dict = row.asDict()\n",
    "    row_dict['Text'] = preprocessing(row_dict['Text'])\n",
    "    row_dict['Tags'] = preprocessing(row_dict['Tags'])\n",
    "    row_dict['Title'] = preprocessing(row_dict['Title'])\n",
    "    newrow = Row(**row_dict)\n",
    "    return newrow\n",
    "\n",
    "\n",
    "dfSchema = StructType([\n",
    "    StructField('Number',IntegerType(), True), \n",
    "    StructField('FileName', StringType(), True), \n",
    "    StructField('Title', StringType(),True), \n",
    "    StructField('Link', StringType(), True),\n",
    "    StructField('ArticleId', IntegerType(), True), \n",
    "    StructField('Date', StringType(), True), \n",
    "    StructField('Views', IntegerType(), True),\n",
    "    StructField('Author', StringType(), True), \n",
    "    StructField('Tags', StringType(), True), \n",
    "    StructField('AmountComments', IntegerType(), True), \n",
    "    StructField('Rating', IntegerType(), True), \n",
    "    StructField('Text', StringType(), True)\n",
    "        ])\n",
    "\n",
    "spark_rdd = df_spark.rdd\n",
    "spark_rdd_new = spark_rdd.map(lambda row: rowwise_function(row))\n",
    "\n",
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new, dfSchema)\n",
    "# spark_rdd_new.show()\n",
    "spark_rdd_new.toPandas().to_csv(\"hot_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "filename = \"hot_dataset.csv\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(filename, inferSchema=True, header=True).toDF('Number','FileName', 'Title', 'Link', 'ArticleId', 'Date', 'Views',\n",
    "       'Author', 'Tags', 'AmountComments', 'Rating', 'Text')\n",
    "\n",
    "ss = SparkSession.builder\n",
    "sparkSession = ss.getOrCreate()\n",
    "sc = sparkSession.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def rowwise_function(row):\n",
    "    row_dict = row.asDict()\n",
    "    row_dict['Text'] = preprocessing(row_dict['Text'])\n",
    "    row_dict['Tags'] = preprocessing(row_dict['Tags'])\n",
    "    row_dict['Title'] = preprocessing(row_dict['Title'])\n",
    "    newrow = Row(**row_dict)\n",
    "    return newrow\n",
    "\n",
    "spark_rdd = df_spark.rdd\n",
    "spark_rdd_new = spark_rdd.map(lambda row: rowwise_function(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new, dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSchema = StructType([\n",
    "    StructField('Number',IntegerType(), True), \n",
    "    StructField('FileName', StringType(), True), \n",
    "    StructField('Title', StringType(),True), \n",
    "    StructField('Link', StringType(), True),\n",
    "    StructField('ArticleId', IntegerType(), True), \n",
    "    StructField('Date', StringType(), True), \n",
    "    StructField('Views', IntegerType(), True),\n",
    "    StructField('Author', StringType(), True), \n",
    "    StructField('Tags', StringType(), True), \n",
    "    StructField('AmountComments', IntegerType(), True), \n",
    "    StructField('Rating', IntegerType(), True), \n",
    "    StructField('Text', StringType(), True)\n",
    "        ])\n",
    "\n",
    "spark_rdd_new = sqlContext.createDataFrame(spark_rdd_new, dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new.toPandas().to_csv(\"hot_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rdd_new.toPandas().to_csv(filename_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прочее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"escape\",\"\\\"\")\n",
    "df_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv(filename, encoding='utf-8') # Prod\n",
    "#frame = frame.where(pd.notnull(frame), None) # Fill NaN with Null\n",
    "\n",
    "\n",
    "dfSchema = StructType([\n",
    "    StructField('Number',IntegerType(), True), \n",
    "    StructField('FileName', StringType(), True), \n",
    "    StructField('Title', StringType(),True), \n",
    "    StructField('Link', StringType(), True),\n",
    "    StructField('ArticleId', StringType(), IntegerType(), True), \n",
    "    StructField('Date', StringType(), True), \n",
    "    StructField('Views', IntegerType(), True),\n",
    "    StructField('Author', StringType(), True), \n",
    "    StructField('Tags', StringType(), True), \n",
    "    StructField('AmountComments', IntegerType(), True), \n",
    "    StructField('Rating', IntegerType(), True), \n",
    "    StructField('Text', StringType(), True)\n",
    "        ])\n",
    "\n",
    "df_s = spark.createDataFrame(frame, dfSchema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
