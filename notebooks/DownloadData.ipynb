{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализация web-crawler для сбора коллекции документов с указанного ресурса. \n",
    "\n",
    "Выбор формата хранения коллекции - MongoDB? Forlder with files? Relation DataBase\n",
    "\n",
    "### Характеристики коллекции:\n",
    "- Объем собранной коллекции не менее 100 тысяч уникальных документов \n",
    "- Размер текста для каждого документа должен быть не менее 2000 символов\n",
    "- Механизм очистки документов от не релевантной информации, например:\n",
    "    - html теги\n",
    "    - ссылки на сторонние или внутренние ресурсы сайта\n",
    "    - ссылки на изображения и видео\n",
    "    - знаки препинания\n",
    "- Выделение признаков документа\n",
    "    - автор\n",
    "    - тематические теги\n",
    "    - рейтинг\n",
    "    - репосты\n",
    "    - дата публикации \n",
    "    - другая метаинформация, предоставляемая ресурсом.\n",
    "\n",
    "### Результатом сдачи является:\n",
    "- код web-crawler\n",
    "- инструмент очистки текстов\n",
    "- исходная коллекция документов\n",
    "- коллекция документов после очистки\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIKABU = \"https://pikabu.ru/\"\n",
    "PIKABU_HOT = \"https://pikabu.ru/\"\n",
    "PIKABU_BEST = \"https://pikabu.ru/best\"\n",
    "PIKABU_FRESH = \"https://pikabu.ru/new\"\n",
    "PIKABU_GROUP = \"https://pikabu.ru/communities\"\n",
    "\n",
    "PIKABU_PAGE = \"https://pikabu.ru/?page={}\"\n",
    "PIKABU_DATE = \"https://pikabu.ru/search?d={}&D={}&page={}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг через BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\138904\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "path_webdriver = 'C:/Users/Public/Programms/chromedriver_win32/chromedriver.exe'\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')  # для открытия headless-браузера\n",
    "\n",
    "browser = webdriver.Chrome(executable_path=path_webdriver, chrome_options=options)\n",
    "print(browser.get(PIKABU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = browser.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(main_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(main_page, 'lxml')\n",
    "table = soup.findChildren('article')\n",
    "# table = soup.findChildren('div', {'class': 'story__content-inner'})\n",
    "\n",
    "# print(len(table))\n",
    "print(table[0].prettify())\n",
    "\n",
    "# print(table[2].find('div', {'class': 'story__content-inner'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pikabu.ru/search?d=1&D=2&page=4\n"
     ]
    }
   ],
   "source": [
    "PIKABU_TEMP = \"https://pikabu.ru/search?d={}&D={}&page={}\"\n",
    "range_days = range(2500, 4600)\n",
    "for d in range_days:\n",
    "    for p in (10):\n",
    "        browser.get(PIKABU_TEMP.format(d,d,p))\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(main_page, 'lxml')\n",
    "        articles = soup.findChildren('article')\n",
    "        for i, article in enumerate(articles):\n",
    "            html_file = open(\"pikabu{}.html\".format(),\"w\")\n",
    "            html_file.write(article)\n",
    "            html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(PIKABU_TEMP.format(3366,3366,1))\n",
    "#browser.get(PIKABU)\n",
    "page = browser.page_source\n",
    "soup = BeautifulSoup(main_page, 'lxml')\n",
    "articles = soup.findChildren('article')\n",
    "for i, article in enumerate(articles):\n",
    "    html_file = open(\"pikabu{}.html\".format(i),\"w\")\n",
    "    html_file.write(str(article))\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pikabu.ru/search?d=3366&D=3366&page=1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIKABU_TEMP.format(3366,3366,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['div', 'div', 'div', 'div', 'svg', 'use', 'div', 'div', 'svg', 'use', 'div', 'div', 'div', 'span', 'div', 'header', 'h2', 'a', 'div', 'div', 'div', 'p', 'p', 'div', 'a', 'a', 'a', 'a', 'a', 'div', 'div', 'a', 'span', 'svg', 'use', 'span', 'div', 'span', 'svg', 'use', 'span', 'div', 'svg', 'use', 'div', 'svg', 'use', 'div', 'div', 'div', 'div', 'a', 'div', 'time', 'a', 'div', 'span', 'img']\n",
      "['div', 'div']\n"
     ]
    }
   ],
   "source": [
    "root = table[0]\n",
    "\n",
    "# перебор всех тегов\n",
    "\n",
    "recursive_childs = [e.name for e in root.recursiveChildGenerator() if e.name is not None]  \n",
    "print(recursive_childs)\n",
    "\n",
    "\n",
    "# При помощи атрибута children можно вывести все дочерние теги.\n",
    "\n",
    "root_childs = [e.name for e in root.children if e.name is not None]\n",
    "print(root_childs)\n",
    "\n",
    "\n",
    "# список всех потомков (дочерних элементов всех уровней) рассматриваемого тега.\n",
    "\n",
    "root_childs = [e.name for e in root.descendants if e.name is not None]\n",
    "# print(root_childs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[моё] Коронавирус Общепит Кризис Список Деньги\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(table[2].find('div', {'class': 'story__tags tags'}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(root.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг через requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как получить user-agent описано здесь https://habr.com/ru/post/280238/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 YaBrowser/20.3.1.197 Yowser/2.5 Safari/537.36'\n",
    "}\n",
    "r = requests.get('https://pikabu.ru//', headers = headers)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"story__content-inner\">\n",
      "<div class=\"story-block story-block_type_text\"><p>Заходит как-то Путин в бар и говорит:</p><p>- Всем пива за счет заведения!</p></div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "articles = soup.find('div', {'class': 'story__content-inner'})\n",
    "print(articles)\n",
    "# lxml \n",
    "tree = html.fromstring(r.text)\n",
    "#film_list_lxml = tree.xpath('//div[@class = \"profileFilmsList\"]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "articles = soup.findChildren('article')\n",
    "\n",
    "print(articles[4].prettify())\n",
    " \n",
    "# tree = html.fromstring(r.text)\n",
    " \n",
    "# tree = html.fromstring(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(articles):\n",
    "    file_name = \"source_html\\pikabu{}.html\".format(i)\n",
    "    with io.open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(a))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 YaBrowser/20.3.1.197 Yowser/2.5 Safari/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "page_range = range(1,150)\n",
    "for p in page_range:\n",
    "    r = requests.get(PIKABU_PAGE.format(p), headers = headers)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    articles = soup.findChildren('article')\n",
    "    for i, article in enumerate(articles):\n",
    "        file_name = \"source_hot_html\\pikabu{}.html\".format(p*10 + i)\n",
    "        with io.open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(article))\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_range = range(1, 15)\n",
    "date_rage = range(2500, 4482)\n",
    "PATH_DATE = \"date\"\n",
    "\n",
    "for p in page_range:\n",
    "    r = requests.get(PIKABU_DATE.format(4482,4482,p), headers = headers)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    articles = soup.findChildren('article')\n",
    "    for i, article in enumerate(articles):\n",
    "        file_name = PATH_DATE + \"\\pikabu{}.html\".format(p*100 + i)\n",
    "        with io.open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(article))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [02:52<00:00, 5781.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "k = 1\n",
    "for i in tqdm(range(1000)):\n",
    "    for j in range(500):\n",
    "        k = j*i*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pikabu.ru/?page=2&twitmode=1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%s?page=%s&twitmode=1' % (PIKABU, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/pikabu1234.html'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/pikabu%s.html\" % 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"ul\", id=\"mylist\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-svg no-js\" dir=\"ltr\" lang=\"ru\">\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# удаление тегов\n",
    "test = BeautifulSoup(r.text, 'lxml')\n",
    "div = test.select_one(\"head\")\n",
    "div.decompose()\n",
    "div = test.select_one(\"body\")\n",
    "div.decompose()\n",
    "print(test.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление всех тегов подобного типа\n",
    "\n",
    "test = BeautifulSoup(r.text, 'lxml')\n",
    "div = test.select(\"meta\")\n",
    "for d in div:\n",
    "    d.decompose()\n",
    "div = test.select(\"link\")\n",
    "for d in div:\n",
    "    d.decompose()\n",
    "# print(test.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    r = requests.get(url, headers=headers)    # Получаем метод Response\n",
    "    return r.text            # Вернем данные объекта text\n",
    "\n",
    "# print(get_html(PIKABU))\n",
    "\n",
    "def get_head(html):\n",
    "    soup = BeautifulSoup(html, 'lxml') #  Создаем сам объект , передаем в него наш код страницы (html) \n",
    "    return soup\n",
    "    \n",
    "soup = get_head(get_html(PIKABU))\n",
    "\n",
    "# print(soup.prettify())\n",
    "## Теперь с помощью метода fine() найдем блок со статьями, <div id=\"section-content\"> . \n",
    "## Уже в этом блоке найдем все теги заголовка <h1> методом fine_all() в которых собственно и содержится название статьи.\n",
    "\n",
    "head = soup.find_all('div', {'class':'auth__field'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(html_page, filename):\n",
    "    html_file = open(filename, \"w\", encoding='UTF-8')\n",
    "    html_file.write(html_page)\n",
    "    html_file.close()\n",
    "    \n",
    "save_html(soup.prettify(),\"mainpage.html\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article():\n",
    "    \n",
    "    def __init__(self, _title, _author, _date, _text, _rating, _tags, _amount_comments, _have_image):\n",
    "        self.title = _title\n",
    "        self.author = _author\n",
    "        self.date = _date\n",
    "        self.text = _text\n",
    "        self.rating = _rating\n",
    "        self.tags = _tags\n",
    "        self.amount_comments = _amount_comments\n",
    "        self.have_image = _have_image\n",
    "        \n",
    "    def showInfo():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = Article(\"title\",\"Sabina\",\"30.03\",\"TestJsonObjectSerialize\",100,['tag1', 'tag2'], 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = json.dumps(art.__dict__) \n",
    "# print(json.dumps(art.__dict__, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create new data_file.json file with write mode using file i/o operation \n",
    "with open('json_file.json', \"w\") as file_write:\n",
    "    # write json data into file\n",
    "    json.dump(art.__dict__, file_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'title', 'author': 'Sabina', 'date': '30.03', 'text': 'TestJsonObjectSerialize', 'rating': 100, 'tags': ['tag1', 'tag2'], 'amount_comments': 10, 'have_image': False}\n"
     ]
    }
   ],
   "source": [
    "with open('json_file.json') as file_object:\n",
    "        # store file data in object\n",
    "    data = json.load(file_object)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n"
     ]
    }
   ],
   "source": [
    "with open('json_file.json') as file_object:\n",
    "        # store file data in object\n",
    "    data = json.load(file_object)\n",
    "    print(data['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
